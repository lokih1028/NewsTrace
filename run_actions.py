#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
NewsTrace GitHub Actions è¿è¡Œå…¥å£
é›¶æˆæœ¬éƒ¨ç½²ä¸“ç”¨è„šæœ¬
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def setup_environment():
    """é…ç½®è¿è¡Œç¯å¢ƒ"""
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    Path("data").mkdir(exist_ok=True)
    Path("data/reports").mkdir(exist_ok=True)
    
    # è®¾ç½®æ•°æ®åº“è·¯å¾„ (SQLite)
    os.environ.setdefault("DATABASE_URL", "sqlite:///data/newstrace.db")
    
    # æ£€æµ‹ LLM é…ç½®
    if os.getenv("GEMINI_API_KEY"):
        os.environ.setdefault("LLM_PROVIDER", "gemini")
        logger.info("âœ… ä½¿ç”¨ Gemini (å…è´¹) ä½œä¸º LLM æä¾›å•†")
    elif os.getenv("OPENAI_API_KEY"):
        os.environ.setdefault("LLM_PROVIDER", "openai")
        logger.info("âœ… ä½¿ç”¨ OpenAI ä½œä¸º LLM æä¾›å•†")
    else:
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° LLM API Key")
    
    logger.info(f"âœ… ç¯å¢ƒé…ç½®å®Œæˆ, æ•°æ®ç›®å½•: {Path('data').absolute()}")


def get_watch_keywords():
    """è·å–å…³æ³¨å…³é”®è¯"""
    keywords_str = os.getenv("WATCH_KEYWORDS", "é»„é‡‘,èŒ…å°,è‹±ä¼Ÿè¾¾,å¤®è¡Œ,GDP")
    return [k.strip() for k in keywords_str.split(",") if k.strip()]


def run_audit_mode():
    """è¿è¡Œå®¡è®¡æ¨¡å¼"""
    logger.info("ğŸ“° å¼€å§‹æ–°é—»é‡‡é›†ä¸å®¡è®¡...")
    
    from src.news_fetcher import NewsFetcher
    from src.audit_engine import AuditEngine
    from src.multi_channel_notifier import MultiChannelNotifier
    from src.semantic_dedup import SemanticDeduplicator
    
    # åˆå§‹åŒ–ç»„ä»¶
    fetcher = NewsFetcher({"provider": "akshare"})
    deduplicator = SemanticDeduplicator(similarity_threshold=0.6)
    
    # æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹© LLM
    provider = os.getenv("LLM_PROVIDER", "gemini")
    audit_config = {
        "provider": provider,
        "model": os.getenv("LLM_MODEL", "gemini-3-flash-preview" if provider == "gemini" else "gpt-4o"),
        "api_key": os.getenv("GEMINI_API_KEY") or os.getenv("OPENAI_API_KEY")
    }
    
    if provider == "openai" and os.getenv("OPENAI_BASE_URL"):
        audit_config["base_url"] = os.getenv("OPENAI_BASE_URL")
    
    auditor = AuditEngine(audit_config)
    notifier = MultiChannelNotifier()
    
    # ğŸ†• è¯­ä¹‰å»é‡ä¸äº‹ä»¶èšåˆ
    event_groups = deduplicator.group_by_event(news_list)
    logger.info(f"ğŸ”„ äº‹ä»¶èšåˆ: åŸå§‹æ–°é—» {original_count} æ¡, è¯†åˆ«å‡º {len(event_groups)} ä¸ªç‹¬ç«‹äº‹ä»¶")
    
    # å®¡è®¡æ¯ä¸ªäº‹ä»¶çš„ä»£è¡¨æ€§æ–°é—»
    results = []
    high_risk_news = []
    
    for event_id, news_group in event_groups.items():
        # é€‰å–ä»£è¡¨æ€§æ–°é—»
        representative_news = deduplicator.get_representative(news_group)
        try:
            result = auditor.audit(representative_news)
            result['_news_title'] = representative_news.get('title', 'æœªçŸ¥æ ‡é¢˜')
            # è®°å½•è¯¥äº‹ä»¶åŒ…å«çš„æ–°é—»æ•°é‡
            result['_event_count'] = len(news_group)
            result['_other_titles'] = [n.get('title') for n in news_group if n != representative_news]
            results.append(result)
            
            audit_result = result.get("audit_result", {})
            risk_level = audit_result.get("risk_level", "Medium")
            
            if risk_level in ["High", "high", "critical", "Critical"]:
                high_risk_news.append({
                    "title": representative_news.get("title"),
                    "risk_level": risk_level,
                    "score": audit_result.get("score", 50),
                    "news_category": audit_result.get("news_category", "neutral"),
                    "core_thesis": audit_result.get("core_thesis") or audit_result.get("one_sentence_conclusion", "N/A"),
                    "event_count": len(news_group)
                })
        except Exception as e:
            logger.error(f"å®¡è®¡å¤±è´¥: {e}")
    
    logger.info(f"âœ… å®¡è®¡å®Œæˆ, è¯†åˆ«ç‹¬ç«‹äº‹ä»¶: {len(results)} ä¸ª, é«˜é£é™©é¢„è­¦: {len(high_risk_news)} æ¡")
    
    # ç”ŸæˆæŠ¥å‘Šï¼ˆä¼ å…¥å»é‡ç»Ÿè®¡ä¿¡æ¯ï¼‰
    dedup_stats = {
        "original": original_count,
        "unique": len(results),
        "duplicates": original_count - len(results)
    }
    report = generate_daily_report(results, high_risk_news, dedup_stats)
    
    # æ¨é€æ—¥æŠ¥
    if notifier.is_available():
        notifier.send(f"ğŸ“Š NewsTrace æ—¥æŠ¥ {datetime.now().strftime('%Y-%m-%d')}", report)
        if high_risk_news:
            logger.info(f"ğŸ“¤ å·²æ¨é€é€šçŸ¥: {len(high_risk_news)} æ¡é«˜é£é™©æ–°é—»")
        else:
            logger.info("ğŸ“¤ å·²æ¨é€æ—¥æŠ¥ (ä»Šæ—¥æ— é«˜é£é™©æ–°é—»)")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = f"data/reports/daily_{datetime.now().strftime('%Y%m%d')}.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    logger.info(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return results


def run_tracking_mode():
    """è¿è¡Œè¿½è¸ªæ¨¡å¼"""
    logger.info("ğŸ“ˆ å¼€å§‹è¿½è¸ªæ›´æ–°...")
    
    # TODO: å®ç°è¿½è¸ªé€»è¾‘
    logger.info("âœ… è¿½è¸ªæ›´æ–°å®Œæˆ")


def get_historical_stats():
    """è·å–å†å²ç»Ÿè®¡æ•°æ®"""
    try:
        from src.database import Database
        db = Database()
        with db.get_connection() as conn:
            cursor = conn.cursor()
            # è·å–æœ€è¿‘ 50 æ¡å·²å®Œæˆè¿½è¸ªçš„è®°å½•
            cursor.execute("""
                SELECT 
                    n.ai_audit_result,
                    mt.price_t0,
                    mt.price_t3
                FROM market_tracking mt
                JOIN news n ON mt.news_id = n.news_id
                WHERE mt.price_t3 IS NOT NULL
                ORDER BY mt.t3_timestamp DESC
                LIMIT 50
            """)
            rows = cursor.fetchall()
            if not rows:
                return None
                
            correct = 0
            for row in rows:
                audit_result, t0, t3 = row
                if isinstance(audit_result, str):
                    audit_result = json.loads(audit_result)
                
                # ç®€å•é€»è¾‘ï¼šçœ‹å¤šä¸”æ¶¨ï¼Œçœ‹ç©ºä¸”è·Œ
                category = audit_result.get("news_category", "neutral")
                if t0 and t3:
                    ret = (t3 - t0) / t0
                    if category == "bullish" and ret > 0.005: correct += 1
                    elif category == "bearish" and ret < -0.005: correct += 1
                    elif category == "neutral" and abs(ret) <= 0.005: correct += 1
            
            return {
                "accuracy": correct / len(rows),
                "sample_count": len(rows)
            }
    except Exception:
        return None


def generate_daily_report(results, high_risk_news, dedup_stats=None):
    """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    stats = get_historical_stats()
    
    # ç»Ÿè®¡åˆ†ç±»
    category_counts = {"bullish": 0, "bearish": 0, "neutral": 0}
    for r in results:
        cat = r.get("audit_result", {}).get("news_category", "neutral")
        category_counts[cat] = category_counts.get(cat, 0) + 1

    report_lines = [
        f"# ğŸ“Š NewsTrace æ¯æ—¥åˆ†ææŠ¥å‘Š",
        f"",
        f"**æ—¥æœŸ**: {current_time}",
        f"",
        f"---",
        f"",
        f"## ğŸ“° åˆ†ææ¦‚è§ˆ",
        f"",
    ]
    
    if dedup_stats:
        report_lines.extend([
            f"- é‡‡é›†åŸå§‹æ–°é—»: `{dedup_stats['original']}` æ¡",
            f"- è¯†åˆ«ç‹¬ç«‹äº‹ä»¶: `{dedup_stats['unique']}` ä¸ª",
            f"- è¯­ä¹‰å»é‡è¿‡æ»¤: `{dedup_stats['duplicates']}` æ¡ (é‡å¤ç‡: {dedup_stats['duplicates']/dedup_stats['original']:.1%})",
            f"- æŠ•èµ„æƒ…ç»ªåˆ†å¸ƒ: ğŸŸ¢åˆ©å¥½ `{category_counts['bullish']}` | ğŸ”´åˆ©ç©º `{category_counts['bearish']}` | âšªä¸­æ€§ `{category_counts['neutral']}`",
        ])
    
    if stats:
        report_lines.append(f"- **ç³»ç»Ÿç½®ä¿¡åº¦**: `{(stats['accuracy']*100):.1f}%` (åŸºäºæœ€è¿‘ {stats['sample_count']} æ¡å†å²å›æµ‹)")
    
    report_lines.append("")
    
    if high_risk_news:
        report_lines.extend([
            f"## âš ï¸ æ ¸å¿ƒå®¡è®¡é¢„è­¦ (Top 5)",
            f""
        ])
        
        for i, news in enumerate(high_risk_news[:5], 1):
            emoji = "ğŸ”´" if news["risk_level"] in ["critical", "Critical"] else "ğŸŸ "
            cat_emoji = "ğŸ“ˆ" if news["news_category"] == "bullish" else "ğŸ“‰" if news["news_category"] == "bearish" else "âš–ï¸"
            group_suffix = f" (ç”± {news['event_count']} ç¯‡æŠ¥é“èšåˆ)" if news['event_count'] > 1 else ""
            
            report_lines.extend([
                f"### {emoji} {i}. {news['title']}{group_suffix}",
                f"",
                f"- **æ€åŠ¿**: `{news['news_category']}` {cat_emoji} | **é€»è¾‘è¯„åˆ†**: `{news['score']}`",
                f"- **æ ¸å¿ƒè®ºç‚¹**: {news.get('core_thesis', 'N/A')}",
                f""
            ])
    else:
        report_lines.extend([
            f"## âœ… å®‰å…¨çŠ¶æ€",
            f"",
            f"æœ¬æ¬¡åˆ†ææœªå‘ç°é«˜é£é™©é¢„è­¦äº‹ä»¶ï¼Œå¸‚åœºå¤„äºä½åˆè°‹æˆ–ä½é£é™©éœ‡è¡çŠ¶æ€ã€‚",
            f""
        ])
    
    # ğŸ“‹ æ‰€æœ‰æ–°é—»æ‘˜è¦
    report_lines.extend([
        f"## ğŸ“‹ æƒ…æŠ¥åº“æ‘˜è¦ (äº‹ä»¶èšåˆ)",
        f""
    ])
    
    for i, result in enumerate(results[:25], 1):
        audit_result = result.get("audit_result", {})
        risk_level = audit_result.get("risk_level", "Medium")
        score = audit_result.get("score", 50)
        category = audit_result.get("news_category", "neutral")
        title = result.get("_news_title", "æœªçŸ¥æ ‡é¢˜")
        event_count = result.get("_event_count", 1)
        conclusion = audit_result.get("one_sentence_conclusion", "")
        
        # é£é™©/æ–¹å‘å›¾æ ‡
        risk_emoji = "ğŸ”´" if risk_level in ["High", "high", "Critical", "critical"] else "ğŸŸ¡"
        cat_tag = "[åˆ©å¥½]" if category == "bullish" else "[åˆ©ç©º]" if category == "bearish" else "[ä¸­æ€§]"
        dup_tag = f" (+{event_count-1}ç¯‡é‡å¤)" if event_count > 1 else ""
        
        report_lines.append(f"{i}. {risk_emoji} **{cat_tag} {title}**{dup_tag}")
        report_lines.append(f"   - é£é™©: `{risk_level}` | è¯„åˆ†: `{score}`")
        if conclusion:
            report_lines.append(f"   - ğŸ’¡ {conclusion}")
    
    report_lines.extend([
        f"",
        f"---",
        f"",
        f"*ç”± NewsTrace è¯­ä¹‰å®¡è®¡å¼•æ“è‡ªåŠ¨ç”Ÿæˆ - [Data-Driven Trust]*"
    ])
    
    return "\n".join(report_lines)


def save_summary(results):
    """ä¿å­˜è¿è¡Œæ‘˜è¦"""
    summary = {
        "run_time": datetime.now().isoformat(),
        "total_analyzed": len(results),
        "high_risk_count": sum(1 for r in results if r.get("risk_level") in ["high", "critical"]),
        "provider": os.getenv("LLM_PROVIDER", "unknown")
    }
    
    with open("data/summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å…¥å£"""
    print("\n" + "=" * 50)
    print("ğŸ“Š NewsTrace é›¶æˆæœ¬éƒ¨ç½²ç‰ˆ")
    print("=" * 50 + "\n")
    
    # é…ç½®ç¯å¢ƒ
    setup_environment()
    
    # è·å–è¿è¡Œæ¨¡å¼
    mode = os.getenv("RUN_MODE", "full")
    logger.info(f"ğŸš€ è¿è¡Œæ¨¡å¼: {mode}")
    
    results = []
    
    try:
        if mode in ("full", "audit"):
            results = run_audit_mode()
        
        if mode in ("full", "tracking"):
            run_tracking_mode()
        
        # ä¿å­˜æ‘˜è¦
        save_summary(results)
        
        logger.info("âœ… NewsTrace è¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
